---
title: "Tarea Individual II"
subtitle: "Ciencia de Datos con R"
format: html
editor: visual
---

## Introducción

El objetivo de esta tarea es aplicar técnicas de **aprendizaje supervisado** utilizando `tidymodels`. Se plantean dos problemas: uno de **regresión** y otro de **clasificación**, utilizando modelos de **árbol de decisión** y **random forest**. Además, se debe interpretar y comunicar los resultados obtenidos.

## Parte 1 -- Regresión

**Dataset:** `ames` (paquete `{modeldata}`)\
**Variable objetivo:** `Sale_Price` (precio de venta de la vivienda)

### 1.1 Preparación de datos

1.1.1. Cargar el dataset `ames` y convertirlo en tibble. Guardar como `ames_data`.

```{r}
#| label: ejercicio_111
library(modeldata)
library(tidyverse)
library(tidymodels)
library(dplyr)
library(rpart.plot)

ames_data <- ames |> as_tibble()
```

1.1.2. Utilizar `Sale_Price` como variable respuesta. Guarda la formula como `formula_ames`.

```{r}
#| label: ejercicio_112
formula_ames <- Sale_Price ~ .
```

1.1.3. Dividir los datos en entrenamiento (80%) y test (20%). Guarda los conjuntos como `ames_train` y `ames_test`.

```{r}
#| label: ejercicio_113
ames_split <- initial_split(ames_data, prop=0.8)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

### 1.2 Entrenamiento del modelo

1.2.1. Definir un modelo de árbol de decisión. Guardar como `tree_ames`.

```{r}
#| label: ejercicio_121
tree_ames <- decision_tree() |>
  set_engine("rpart") |>
  set_mode("regression")
```

1.2.2. Entrenar el modelo utilizando `fit()`. Guardar el modelo entrenado como `fit_tree_ames`.

```{r}
#| label: ejercicio_122
fit_tree_ames <- tree_ames |>
  fit(formula_ames, data=ames_train)
```

### 1.3 Evaluación del modelo

1.3.1. Predecir sobre el conjunto de test. Guardar las predicciones como `predictions_ames`.

```{r}
#| label: ejercicio_131
predictions_ames <- predict(fit_tree_ames, ames_test)
```

1.3.2. Calcular **RMSE** y **R²** tanto para train y test. En el caso de `ames_train` guardar como `metrics_train_ames` y en el caso de `ames_test` como `metrics_test_ames`.

```{r}
#| label: ejercicio_132
multi_metric <- metric_set(rmse, rsq)
metrics_train_ames <- multi_metric(
  predict(fit_tree_ames, ames_train) |>
    bind_cols(ames_train |> select(Sale_Price)),
  truth=Sale_Price,
  estimate=.pred
)
metrics_test_ames <- multi_metric(
  predictions_ames |>
    bind_cols(ames_test |> select(Sale_Price)),
  truth=Sale_Price,
  estimate=.pred
)
```

1.3.3. Graficar valores reales vs. estimados (`Sale_Price`) con línea de identidad, utilizando los datos de entrenamiento. Mostrar el gráfico y guardarlo como `plot_tree_ames`.

```{r}
#| label: ejercicio_133
plot_tree_ames <- predictions_ames |>
  bind_cols(ames_test |> select(Sale_Price)) |>
  ggplot(aes(x=Sale_Price, y=.pred)) +
  geom_point() +
  geom_abline() +
  theme_bw()

plot_tree_ames
```

1.3.4. Interpretar las métricas y el gráfico. (En este caso, no es necesario un chunk de código, pero sí una breve explicación en el texto).

Las predicciones del modelo tienen una diferencia promedio de 38678 y 44303 para train y test respectivamente con un R2 de 0.77 y 0.68 respectivamente. Es decir, el modelo explica una buena proporción de la variabilidad de los datos pero aún hay lugar para un ajuste mejor y esto se ve en que, en promedio, hay grandes diferencias entre el valor esperado y el predicho.

1.3.5. Evaluar si hay sobreajuste o subajuste. Esta respuesta es libre y puede realizar el códogo que considere necesario para evaluar el modelo.

Si bien el rendimiento de un modelo también depende del dominio donde se está trabajando (los puntos de corte de una buena métrica o no son más o menos laxos), se podria decir que el modelo no presenta señales claras de sobreajuste o subajuste. El modelo es relativamente bueno en entrenamiento y generaliza relativamente bien en el conjunto de validación donde no cae tanto la métrica con respecto a las de entrenamiento, aunque si tiene márgen de mejora y, para algún dominio, se podria decir que no está ajustando bien.

### 1.4 Interpretación del árbol

1.4.1. Visualizar el árbol. Guardar el gráfico como `plot_tree_ames_final`.

```{r}
#| label: ejercicio_141
plot_tree_ames_final <- rpart.plot(fit_tree_ames$fit, roundint=F)
plot_tree_ames_final
```

1.4.2. Interpretar brevemente las decisiones del árbol.

Garage_Car es una variable fuerte al momento de predecir el precio de venta. Utiliza hasta 5 niveles de distintas variables predictoras del precio, entre ellas: First_Flr_SF, Neighborhood, Gr_Liv_Area, etc.

------------------------------------------------------------------------

## Parte 2 - Clasificación

**Dataset:** `attrition` (paquete `{modeldata}`)\
**Variable objetivo:** `Attrition` (abandono laboral: "Yes" o "No")

### 2.1 Preparación de datos

2.1.1. Cargar el dataset `attrition` y convertirlo en tibble. Guardar como `attrition_data`.

```{r}
#| label: ejercicio_211
attrition_data <- attrition |> as_tibble()
```

2.1.2. Asegurarse de que `Attrition` sea un factor. Sobreescribir la variable si es necesario.

```{r}
#| label: ejercicio_212
attrition_data <- attrition_data |> mutate(Attrition = factor(Attrition))
```

2.1.3. Dividir los datos en entrenamiento (80%) y test (20%) manteniendo la misma proporción de clases en ambos datasets. Guarda los conjuntos como `attrition_train` y `attrition_test`.

```{r}
#| label: ejercicio_213
attrition_split <- initial_split(attrition_data, prop=0.80, strata=Attrition)
attrition_train <- training(attrition_split)
attrition_test <- testing(attrition_split)
```

### 2.2 Entrenamiento del modelo

2.2.1. Definir un modelo de random forest. Llevar a cabo la definición utilizando `rand_forest()` y `set_engine()`. Guardar como `rf_attrition`.

```{r}
#| label: ejercicio_221
rf_attrition <- rand_forest(trees=1000) |>
  set_engine('ranger', importance="impurity") |>
  set_mode("classification")
```

2.2.2. Entrenar el modelo. Llevar a cabo el entrenamiento utilizando `fit()`. Utilizar la fórmula `Attrition ~ .` para incluir todas las variables predictoras. Guardar el modelo entrenado como `fit_rf_attrition`.

```{r}
#| label: ejercicio_222
fit_rf_attrition <- rf_attrition |> fit(Attrition ~ ., data=attrition_train)
```

### 2.3 Evaluación del modelo

2.3.1. Predecir sobre el conjunto de test. Guardar las predicciones como `predictions_attrition`.

```{r}
#| label: ejercicio_231
predictions_attrition <- attrition_test |>
  select(Attrition) |>
  bind_cols(predict(fit_rf_attrition, attrition_test))
```

2.3.2. Calcular **accuracy**, **precision**, **recall** y **F1**. Guardar las métricas de entrenamiento como `metrics_train_attrition` y las de test como `metrics_test_attrition`.

```{r}
#| label: ejercicio_232
classif_metrics_set <- metric_set(accuracy, precision, recall, f_meas)
metrics_train_attrition <- classif_metrics_set(
  attrition_train |>
  select(Attrition) |>
  bind_cols(predict(fit_rf_attrition, attrition_train)),
  truth=Attrition,
  estimate=.pred_class
)
metrics_test_attrition <- classif_metrics_set(predictions_attrition, truth=Attrition, estimate=.pred_class)
```

2.3.3. Visualizar matriz de confusión con `autoplot(type = "heatmap")`. Imprimir el gráfico y guardarlo como `plot_confusion_attrition`.

```{r}
#| label: ejercicio_233
plot_confusion_attrition <- conf_mat(predictions_attrition, truth=Attrition, estimate=.pred_class) |>
  autoplot(type="heatmap") +
  scale_fill_gradient(low="skyblue", high="steelblue")
plot_confusion_attrition
```

2.3.4. Visualizar variables importantes con `vip()`. Imprimir el gráfico y guardarlo como `plot_vip_attrition`.

```{r}
#| label: ejercicio_234
plot_vip_attrition <- fit_rf_attrition |> vip::vip()
plot_vip_attrition
```

2.3.5. Interpretar las métricas y los errores. (En este caso, no es necesario un chunk de código, pero sí una breve explicación en el texto).

El modelo ha memorizado todos los verdaderos positivos (recall=1) del conjunto de entrenamiento y ha sabido clasificar correctamente casi todos los casos que se traduce en las métricas altas que luego caen en el conjunto de test donde comete mas falsos positivos (aunque mantiene la buena metrica en los positivos) pero ha tenido varios casos de una mala clasificación en la clase de menor representitividad y esto se traduce en la baja de \~10% en la precision y accuracy.

2.3.6. Evaluar si hay sobreajuste o subajuste. Esta respuesta es libre y puede realizar el código que considere necesario para evaluar el modelo.

Hay indicios de sobreajuste del modelo ya que predice prácticamente de memoria la clase con mayor representividad en el conjunto de entrenamiento pero falla en generalizar con nuevos datos donde la mayoria son falsos positivos especialmente en la clase con menor representividad.

------------------------------------------------------------------------

## Entrega

-   **La fecha limite de entrega es el 20 de junio de 2025.**

-   Las respuestas deben estar en este mismo archivo `.qmd`, el contenido deber ser completamente reproducible, es decir, cada `chunk` debe de funcionar sin errores para poder replicar los resultados.

-   No se aceptan archivos `.Rmd` o `.R` para la entrega. Solamente subir al repositorio el archivo `.qmd`con las respuestas.

-   Cada respuesta del ejercicio debe estar en el chunk correspondiente, no borrar la etiqueta del chunk `#| label: ejercicio_XX`.

-   Puede realizar pasos intermedios los que sean necesarios dentro del chunk pero debe de respetar el nombre del objeto final en el caso que se indique.

-   Los gráficos deben ser guardados en objetos y luego impresos en el caso que se indique que lo almacenen en un objeto. En el caso que no se indique, pueden ser impresos directamente.

-   Para comenzar la tarea deben de ir al siguiente link: GitHub Classroom. Una vez allí les va a pedir que indiquen su cuenta de GitHub y luego les va a crear un repositorio en su cuenta. Una vez creado el repositorio, deben de clonar el repositorio en su computadora y abrirlo con RStudio
